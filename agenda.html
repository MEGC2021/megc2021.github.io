<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>MEGC2021</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo"><strong>MEGC2021</strong> </a>
								</header>

							<!-- Content -->
								<section>

									<header class="main">
										<h2>Facial Micro-Expression (FME) Workshop and Challenge 2021</h2>
									</header>

									<h2> - Advanced techniques for Facial Expressions Generation and Spotting</h2>

									<p> <span style="color: red; ">Update:</span> Download <a href="images/call for paper_fme2021.pdf"> Call for Papers (pdf version)</a>.</p>

									<p>
										Micro-facial expressions (MEs) are involuntary movements of the face that occur spontaneously in a high-stakes environment. Computational analysis and automation of tasks on micro-expressions is an emerging area in face research, with a strong interest appearing from 2014. Only recently, the availability of a few spontaneously induced facial micro-expression datasets has provided the impetus to advance further from the computational aspect. Particularly comprehensive are two state-of-the art FACS coded datasets: CASME II and SAMM. While much research has been done on these datasets individually, there has been no attempts to introduce a more rigorous and realistic evaluation to work done in this domain. This is the inaugural workshop in this area of research, with the aim of promoting interactions between researchers and scholars from within this niche area of research, and also including those from broader, general areas of expression and psychology research.

									</p>
								</section>
							<section>
								<header class="main">
										<h2>Agenda</h2>
								</header>


								<h4>This workshop has two main agenda:</h4>

								<OL>
									<LI><b>To solicit original works that address a variety of challenges of Facial Expressions research, but not limited to:</b>
										<a name="workshop"></a>
										<UL>
											<li>Facial expressions (both micro- and macro-expressions) detection/spotting</li>
											<li>Facial expressions recognition </li>
											<li>Multi-modal micro-expression analysis, combining such as depth information, heart rate signal etc. </li>
											<li>FME feature representation and computational analysis </li>
											<li>Unified FME spot-and-recognize schemes </li>
											<li>Deep learning techniques for FMEs detection and recognition</li>
											<li>New objective classes for FMEs analysis </li>
											<li>New FMEs datasets Facial expressions data synthesis</li>
											<li>Psychology of FMEs research </li>
											<li>Facial Action Unit (AU) detection and recognition </li>
											<li>Emotion recognition using Aus</li>
											<li>FME Applications </li>
										</UL>
									</LI>

									<Li><b>To organize a Facial Micro-Expression (FME) Challenge for facial micro-expression research, involving FME generation and spotting. </b>
										<br />
										<a name="challenge"></a>
										<br />
										<ul class="actions">
											<li><a href="#Detail" class="button small">Learn More</a></li>
										</ul>
									</Li>
								</OL>


							</section>
							<!-- Content -->
							<section>

								<header class="main">
									<h2>Description of Challenge tasks</h2>
								</header>
								<a name="Detail"></a>

								<!--<h2> - Advanced techniques for Facial Expressions Generation and Spotting</h2> -->
								<h3>Databases</h3>

								<p>
									These are three state-of-the-art datasets for <b>ME recognition task</b>: the Chinese Academy of Sciences Micro-Expression Database II (CASME II) with 247 FMEs at 200 fps, SMIC-E with 157 FMEs at 100 fps and Spontaneous Facial Micro-Movement Dataset (SAMM) with 159 FMEs at 200 fps. </p>
								<p>Moreover, researchers begin to put efforts on the databases which contain long video sequences: the SAMM Long Videos with 147 long videos at 200 fps (average duration: 35.5s), the CAS(ME)<sup>2</sup> with 97 long videos at 30 fps (average duration: 148s). In this challenge, we use CAS(ME)<sup>2</sup> and SAMM Long Videos for the task of <b>micro- and macro-expression spotting</b>.
								</p>

								<h3>Facial micro-expression generation task:</h3>
								<p>The goal of this task is to generate specific micro-expression (source) on the given template faces (target). </p>
								<LI>Guidelines:  download
                            		<a href="images/Facial_Micro_Expression__FME__Challenge_2021_generation_guideline.pdf">file here</a></LI>
								<LI>Source and Target samples:
									<a href="images/MEGC2021_generation.zip">file here</a></LI>. Please apply to download the complete databases according to the description in the guideline document.
								<LI>As the submission results will be evaluated by three experts, there are no baseline method and result for the generation task.</LI>
								<br />

								<h3>Facial macro- and micro-expression spotting task </h3>
								<LI>Guidelines: download
                            <a href="images/Facial_Micro_Expression__FME__Challenge_2021_spotting_guideline.pdf">file here</a></LI>
								<LI>Baseline method:
									<br />
									Please cite:
									<br />
Yap, C.H., Yap, M.H., Davison, A.K., Cunningham, R. (2021), Efficient Lightweight 3D-CNN using Frame Skipping and Contrast Enhancement for Facial Macro- and Micro-expression Spotting, arXiv:2105.06340 [cs.CV], <a href="https://arxiv.org/abs/2105.06340">https://arxiv.org/abs/2105.06340</a>.
									</LI>
								<LI>Baseline code: As the paper of the baseline method is currently under reviewed, the code will be shared when the paper is accepted.  </LI>
								<LI>Baseline results:  </LI>
								<table border="2">
								<tr>
									<td>&nbsp;</td>
									<td>MaE</td>
									<td>ME</td>
									<td>Combined</td>

								</tr>
								<tr>
									<td>SAMM-LV</td>
									<td>0.1863</td>
									<td>0.0409</td>
									<td>0.1193</td>

								</tr>
								<tr>
									<td>CAS(ME)<sup>2</sup>-cropped <span style="color: red; "> (for final evaluation)</span> </td>
									<td>0.0401</td>
									<td>0.0118</td>
									<td>0.0304</td>
								</tr>
								<tr>
									<td>CAS(ME)<sup>2</sup> (could be used as reference)</td>
									<td>0.0686</td>
									<td>0.0119</td>
									<td>0.0497</td>
								</tr>
								</table>
								<LI>Frequently Asked Questions:</LI>
								<OL>
											<li>Q: How to deal with the spotted intervals with overlap? <br />
												A: We consider that each ground-truth interval corresponds to at most one single spotted interval. If your algorithm detects multiple  with overlap, you should merge them into an optimal interval. The fusion method is also part of your algorithm, and the final result evaluation only cares about the optimal interval obtained.
											</li>
									<li>Q: How to validate the algorithm and obtain the final result when using machine learning / deep learning methods? Will there be a test set release? <br />
										A: As the data size for micro-expression is small, all the samples from CAS(ME)<sup>2</sup>-cropped and SAMM-LV should be used for result evaluation. And the common validation method is Leave-one-Subject-out cross validation.
											</li>

										</OL>


								<br />

							</section>


						</div>
					</div>

					<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
								<section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section>

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="index.html">Homepage</a></li>
										<li><a href="agenda.html">Call for Papers</a></li>
										<!--
										<li>
											<span class="opener">Call for Papers</span>
											<ul>
												<li><a href="#workshop">Workshop</a></li>
												<li><a href="#challenge">Challenge</a></li>

											</ul>
										</li>
										-->
										<li><a href="submission.html">Submission</a></li>
										<li><a href="organisers.html">Organisers</a></li>
										<li><a href="program.html">Program</a></li>
										<li><a href="review.html">Continuity</a></li>
									</ul>
								</nav>



							<!-- Section -->
								<section>
									<header class="major">
										<h2>Get in touch</h2>
									</header>
									<ul class="contact">
										<li class="icon solid fa-envelope"><a href="#">lijt@psych.ac.cn</a></li>
									</ul>
								</section>
							<footer id="footer">
									<p class="copyright">&copy; MEGC2021. All rights reserved. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
							</footer>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
